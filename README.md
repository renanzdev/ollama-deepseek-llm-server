
# ğŸ§  Flask API com DeepSeek-R1 via Ollama

Esta API foi desenvolvida utilizando Flask em Python e conecta-se ao modelo de linguagem **DeepSeek-R1** por meio da plataforma [Ollama](https://ollama.com/). O principal objetivo deste projeto Ã© realizar inferÃªncias de LLMs (Modelos de Linguagem de Grande Escala) de maneira simples e eficiente, de forma local, com uma interface de usuÃ¡rio (UI) bÃ¡sica e resposta em tempo real (stream).

## âš™ï¸ PrÃ©-requisitos

Antes de rodar o projeto, assegure-se de ter os seguintes itens instalados:

- [Python 3.8+](https://www.python.org/downloads/)
- [Ollama](https://ollama.com/)
- O modelo `deepseek-r1` baixado no Ollama

## ğŸš€ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio**

```bash
git clone https://github.com/renanzdev/ollama-deepseek-llm-server.git
cd ollama-deepseek-llm-server
```

2. **Instale as dependÃªncias**

```bash
pip install -r requirements.txt
```

3. **Instale e inicie o Ollama**
Se vocÃª ainda nÃ£o tem o Ollama instalado, utilize o seguinte comando para instalar:

```bash
# No Linux via curl
curl -fsSL https://ollama.com/install.sh | sh
```

4. **Baixe o modelo DeepSeek-R1**

Se o Ollama jÃ¡ estiver instalado, basta baixar o modelo com o comando:

```bash
ollama pull deepseek-r1:latest
```

## ğŸ§ª Executando a API

ApÃ³s a instalaÃ§Ã£o, basta rodar o script `app.py` para iniciar a API:

```bash
python app.py
```

## ğŸ“ Estrutura do projeto

A estrutura do projeto Ã© a seguinte:

```bash
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ ollama.py          # ServiÃ§o para interagir com o modelo Ollama
â”œâ”€â”€ app.py                 # Arquivo principal para execuÃ§Ã£o da API
â”œâ”€â”€ requirements.txt       # Arquivo contendo as dependÃªncias do projeto
â””â”€â”€ README.md              # Este arquivo
```

## ğŸ‘¤ CrÃ©ditos

Desenvolvido por **Renan RodrÃ­guez**. Sinta-se Ã  vontade para contribuir ou utilizar este projeto em seus prÃ³prios projetos.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob os termos da **MIT License**.
